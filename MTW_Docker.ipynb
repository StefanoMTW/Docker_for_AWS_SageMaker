{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch.nn\n",
      "ERROR: No matching distribution found for torch.nn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch.nn.functional\n",
      "ERROR: No matching distribution found for torch.nn.functional\n",
      "UsageError: Line magic function `%torch.optim` not found.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install torch.nn\n",
    "%pip install torch.nn.functional\n",
    "%torch.optim\n",
    "%torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gzip\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode binary data from SM_CHANNEL_TRAINING\n",
    "# Decode and preprocess data\n",
    "# Create map dataset\n",
    "\n",
    "\n",
    "def normalize(x, axis):\n",
    "    eps = np.finfo(float).eps\n",
    "    mean = np.mean(x, axis=axis, keepdims=True)\n",
    "    # avoid division by zero\n",
    "    std = np.std(x, axis=axis, keepdims=True) + eps\n",
    "    return (x - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensor(data_dir, images_file, labels_file):\n",
    "    \"\"\"Byte string to torch tensor\"\"\"\n",
    "    with gzip.open(os.path.join(data_dir, images_file), \"rb\") as f:\n",
    "        images = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28, 28).astype(np.float32)\n",
    "\n",
    "    with gzip.open(os.path.join(data_dir, labels_file), \"rb\") as f:\n",
    "        labels = np.frombuffer(f.read(), np.uint8, offset=8).astype(np.int64)\n",
    "\n",
    "    # normalize the images\n",
    "    images = normalize(images, axis=(1, 2))\n",
    "\n",
    "    # add channel dimension (depth-major)\n",
    "    images = np.expand_dims(images, axis=1)\n",
    "\n",
    "    # to torch tensor\n",
    "    images = torch.tensor(images, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.int64)\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(Dataset):\n",
    "    def __init__(self, data_dir, train=True):\n",
    "\n",
    "        if train:\n",
    "            images_file = \"train-images-idx3-ubyte.gz\"\n",
    "            labels_file = \"train-labels-idx1-ubyte.gz\"\n",
    "        else:\n",
    "            images_file = \"t10k-images-idx3-ubyte.gz\"\n",
    "            labels_file = \"t10k-labels-idx1-ubyte.gz\"\n",
    "\n",
    "        self.images, self.labels = convert_to_tensor(data_dir, images_file, labels_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    use_cuda = args.num_gpus > 0\n",
    "    device = torch.device(\"cuda\" if use_cuda > 0 else \"cpu\")\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        MNIST(args.train, train=True), batch_size=args.batch_size, shuffle=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        MNIST(args.test, train=False), batch_size=args.test_batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    net = Net().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        net.parameters(), betas=(args.beta_1, args.beta_2), weight_decay=args.weight_decay\n",
    "    )\n",
    "\n",
    "    logger.info(\"Start training ...\")\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        net.train()\n",
    "        for batch_idx, (imgs, labels) in enumerate(train_loader, 1):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            output = net(imgs)\n",
    "            loss = loss_fn(output, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(imgs),\n",
    "                        len(train_loader.sampler),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # test the model\n",
    "        test(net, test_loader, device)\n",
    "\n",
    "    # save model checkpoint\n",
    "    save_model(net, args.model_dir)\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            output = model(imgs)\n",
    "            test_loss += F.cross_entropy(output, labels, reduction=\"sum\").item()\n",
    "\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    logger.info(\n",
    "        \"Test set: Average loss: {:.4f}, Accuracy: {}/{}, {})\\n\".format(\n",
    "            test_loss, correct, len(test_loader.dataset), 100.0 * correct / len(test_loader.dataset)\n",
    "        )\n",
    "    )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_dir):\n",
    "    logger.info(\"Saving the model\")\n",
    "    path = os.path.join(model_dir, \"model.pth\")\n",
    "    torch.save(model.cpu().state_dict(), path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data and model checkpoints directories\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        default=64,\n",
    "        metavar=\"N\",\n",
    "        help=\"input batch size for training (default: 64)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test-batch-size\",\n",
    "        type=int,\n",
    "        default=1000,\n",
    "        metavar=\"N\",\n",
    "        help=\"input batch size for testing (default: 1000)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\", type=int, default=1, metavar=\"N\", help=\"number of epochs to train (default: 1)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning-rate\",\n",
    "        type=float,\n",
    "        default=0.001,\n",
    "        metavar=\"LR\",\n",
    "        help=\"learning rate (default: 0.01)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--beta_1\", type=float, default=0.9, metavar=\"BETA1\", help=\"beta1 (default: 0.9)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--beta_2\", type=float, default=0.999, metavar=\"BETA2\", help=\"beta2 (default: 0.999)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--weight-decay\",\n",
    "        type=float,\n",
    "        default=1e-4,\n",
    "        metavar=\"WD\",\n",
    "        help=\"L2 weight decay (default: 1e-4)\",\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", type=int, default=1, metavar=\"S\", help=\"random seed (default: 1)\")\n",
    "    parser.add_argument(\n",
    "        \"--log-interval\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "        metavar=\"N\",\n",
    "        help=\"how many batches to wait before logging training status\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--backend\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"backend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\",\n",
    "    )\n",
    "\n",
    "    # Container environment\n",
    "    parser.add_argument(\"--hosts\", type=list, default=json.loads(os.environ[\"SM_HOSTS\"]))\n",
    "    parser.add_argument(\"--current-host\", type=str, default=os.environ[\"SM_CURRENT_HOST\"])\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ[\"SM_CHANNEL_TRAINING\"])\n",
    "    parser.add_argument(\"--test\", type=str, default=os.environ[\"SM_CHANNEL_TESTING\"])\n",
    "    parser.add_argument(\"--num-gpus\", type=int, default=os.environ[\"SM_NUM_GPUS\"])\n",
    "\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    train(args)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "183bbf6827d058c2a2fb0f4acdc0420849dda2b4380af0e437e38c64d798d8b7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
